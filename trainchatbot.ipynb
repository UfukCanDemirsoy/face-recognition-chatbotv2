{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47ea2b9-a5dd-4b32-be94-a8d6c19b2885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.device_count()) \n",
    "print(torch.cuda.get_device_name(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13588746-5402-4c60-9727-8cb95589ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ufuk\\anaconda3\\envs\\environment2\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Model ve tokenizer'ı yükle\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b207289-bfc8-4dda-9b60-8c6737b4cd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# GPT-2\\'ye padding token ekle\\ntokenizer.pad_token = tokenizer.eos_token\\n\\n# 2. Fine-tuning yapmak için bir veri seti yükle (Wikitext)\\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\\n\\n# 3. Veri setini eğitim ve değerlendirme (validation) olarak ikiye böl\\ntrain_dataset = dataset[\"train\"]\\nvalidation_dataset = dataset[\"validation\"]\\n\\n# 4. Veriyi GPT-2 için tokenle ve \\'labels\\' ekle\\ndef tokenize_function(examples):\\n    tokens = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # \\'labels\\' olarak \\'input_ids\\' kullan\\n    return tokens\\n\\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\\ntokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\\n\\n# 5. Eğitim ayarları\\ntraining_args = TrainingArguments(\\n    output_dir=\"./results\",                    # Çıktı dizini\\n    evaluation_strategy=\"epoch\",               # Her epoch sonunda değerlendirme\\n    per_device_train_batch_size=2,             # Her cihazda batch boyutu\\n    per_device_eval_batch_size=2,              # Her cihazda evaluation batch boyutu\\n    num_train_epochs=3,                        # Eğitim epoch sayısı\\n    save_steps=10_000,                         # Model kaydetme adımı\\n    save_total_limit=2,                        # Maksimum 2 checkpoint sakla\\n    logging_dir=\\'./logs\\',                      # Log dosyası dizini\\n    logging_steps=200,                         # Her 200 adımda bir log yaz\\n)\\n\\n# 6. Eğitici tanımla\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_train_dataset,\\n    eval_dataset=tokenized_validation_dataset,  # Değerlendirme veri seti ekleniyor\\n)\\n\\n# 7. Modeli eğit\\ntrainer.train()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# GPT-2'ye padding token ekle\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Fine-tuning yapmak için bir veri seti yükle (Wikitext)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# 3. Veri setini eğitim ve değerlendirme (validation) olarak ikiye böl\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"]\n",
    "\n",
    "# 4. Veriyi GPT-2 için tokenle ve 'labels' ekle\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # 'labels' olarak 'input_ids' kullan\n",
    "    return tokens\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 5. Eğitim ayarları\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                    # Çıktı dizini\n",
    "    evaluation_strategy=\"epoch\",               # Her epoch sonunda değerlendirme\n",
    "    per_device_train_batch_size=2,             # Her cihazda batch boyutu\n",
    "    per_device_eval_batch_size=2,              # Her cihazda evaluation batch boyutu\n",
    "    num_train_epochs=3,                        # Eğitim epoch sayısı\n",
    "    save_steps=10_000,                         # Model kaydetme adımı\n",
    "    save_total_limit=2,                        # Maksimum 2 checkpoint sakla\n",
    "    logging_dir='./logs',                      # Log dosyası dizini\n",
    "    logging_steps=200,                         # Her 200 adımda bir log yaz\n",
    ")\n",
    "\n",
    "# 6. Eğitici tanımla\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_validation_dataset,  # Değerlendirme veri seti ekleniyor\n",
    ")\n",
    "\n",
    "# 7. Modeli eğit\n",
    "trainer.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8da8ef-cba2-4e3c-baa0-5d8166be1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli kaydetme\n",
    "#trainer.save_model(\"./trained_model\")  # Model ./trained_model klasörüne kaydedilecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f6406f-9f10-4174-bd4f-413a4a770003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c003b4-1a17-4104-9448-189452987774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0032846-7773-4716-a84b-bc87c661336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohbet botuna hoş geldiniz! Su anda sadece ingilizce sohbet gecerlidir. (Çıkmak için 'exit' yazın)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  Oh hello my lord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ufuk\\anaconda3\\envs\\environment2\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Oh hello my lord , my sweetheart . I am your sweetest , the sweet one . \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  Are u a chatbot?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Are u a chatbot? \" is a common question in chat rooms . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat_with_model():\n",
    "    print(\"Sohbet botuna hoş geldiniz! Su anda sadece ingilizce sohbet gecerlidir. (Çıkmak için 'exit' yazın)\")\n",
    "    \n",
    "    while True:\n",
    "        # Kullanıcıdan bir soru al\n",
    "        user_input = input(\"Sen: \")\n",
    "        \n",
    "        # Çıkış için 'exit' komutunu kontrol et\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Sohbet sona erdi.\")\n",
    "            break\n",
    "\n",
    "        # Kullanıcı girdisini tokenleştir\n",
    "        inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Model ile yanıt üret\n",
    "        output = model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_length=100,  # Üretilecek maksimum token sayısı\n",
    "            num_return_sequences=1,  # Kaç yanıt üretileceği\n",
    "            no_repeat_ngram_size=2,  # Aynı cümlenin tekrarını önlemek için\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Tokenleri metne dönüştür\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Modelin yanıtını göster\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Sohbet başlat\n",
    "chat_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db242bf-0b79-47e6-8518-7e5cfd0cd908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
